{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec289cd",
   "metadata": {},
   "source": [
    "# Serialize Datasets & Dataloaders\n",
    "\n",
    "This notebook will create datasets and dataloaders for a single 17lands set. \n",
    "\n",
    "These datasets can be used for model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "07f3556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_columns = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3546117a",
   "metadata": {},
   "source": [
    "## ETL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef6725c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is better. \n",
    "# compressed_csv_path = \"../data/BLB/draft_data_public.BLB.PremierDraft.csv.gz\"\n",
    "csv_path = \"../data/BLB_new/draft_data_public.BLB.PremierDraft.csv\" # 5 weeks of data - can not be loaded by pandas. \n",
    "\n",
    "\n",
    "# t0 = time.time()\n",
    "# raw_draft_chunk = pd.read_csv(csv_path)\n",
    "# raw_draft_chunk = pd.read_csv(compressed_csv_path, compression=\"gzip\")\n",
    "# print(f\"Time to read compressed: {time.time() - t0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "610bbf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_basics(draft_chunk):\n",
    "    # Remove basic lands from draft. \n",
    "    basic_names = [\"Forest\", \"Island\", \"Mountain\", \"Plains\", \"Swamp\"]\n",
    "    columns_to_drop = [\"pack_card_\" + b for b in basic_names] + [\"pool_\" + b for b in basic_names]\n",
    "    draft_chunk = draft_chunk.drop(columns=columns_to_drop)\n",
    "    draft_chunk = draft_chunk[~draft_chunk[\"pick\"].isin(basic_names)]\n",
    "    return draft_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "97d33afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed one-time computation.\n",
      "processed a chunk, t= 3 s\n",
      "processed a chunk, t= 5 s\n",
      "processed a chunk, t= 6 s\n",
      "processed a chunk, t= 8 s\n",
      "processed a chunk, t= 9 s\n",
      "processed a chunk, t= 11 s\n",
      "processed a chunk, t= 12 s\n",
      "processed a chunk, t= 14 s\n",
      "processed a chunk, t= 16 s\n",
      "processed a chunk, t= 17 s\n",
      "processed a chunk, t= 19 s\n",
      "processed a chunk, t= 20 s\n",
      "processed a chunk, t= 22 s\n",
      "processed a chunk, t= 23 s\n",
      "processed a chunk, t= 25 s\n",
      "processed a chunk, t= 28 s\n",
      "processed a chunk, t= 30 s\n",
      "processed a chunk, t= 31 s\n",
      "processed a chunk, t= 33 s\n",
      "processed a chunk, t= 35 s\n",
      "processed a chunk, t= 36 s\n",
      "processed a chunk, t= 38 s\n",
      "processed a chunk, t= 39 s\n",
      "processed a chunk, t= 41 s\n",
      "processed a chunk, t= 43 s\n",
      "processed a chunk, t= 44 s\n",
      "processed a chunk, t= 46 s\n",
      "processed a chunk, t= 48 s\n",
      "processed a chunk, t= 49 s\n",
      "processed a chunk, t= 51 s\n",
      "processed a chunk, t= 53 s\n",
      "processed a chunk, t= 54 s\n",
      "processed a chunk, t= 56 s\n",
      "processed a chunk, t= 58 s\n",
      "processed a chunk, t= 60 s\n",
      "processed a chunk, t= 62 s\n",
      "processed a chunk, t= 64 s\n",
      "processed a chunk, t= 65 s\n",
      "processed a chunk, t= 67 s\n",
      "processed a chunk, t= 69 s\n",
      "processed a chunk, t= 71 s\n",
      "processed a chunk, t= 73 s\n",
      "processed a chunk, t= 75 s\n",
      "processed a chunk, t= 76 s\n",
      "processed a chunk, t= 78 s\n",
      "processed a chunk, t= 80 s\n",
      "processed a chunk, t= 82 s\n",
      "processed a chunk, t= 83 s\n",
      "processed a chunk, t= 85 s\n",
      "processed a chunk, t= 87 s\n",
      "processed a chunk, t= 89 s\n",
      "processed a chunk, t= 91 s\n",
      "processed a chunk, t= 93 s\n",
      "processed a chunk, t= 94 s\n",
      "processed a chunk, t= 96 s\n",
      "processed a chunk, t= 98 s\n",
      "processed a chunk, t= 100 s\n",
      "processed a chunk, t= 102 s\n",
      "processed a chunk, t= 104 s\n",
      "processed a chunk, t= 105 s\n",
      "processed a chunk, t= 106 s\n",
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((978335, 271), (978335, 271), (978335, 271))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataloaders in memory-efficient fashion\n",
    "pack_chunks, pool_chunks, pick_chunks = [], [], []\n",
    "chunk_size = 100000 \n",
    "min_date_str = '9999-01-01 00:00:00' # First date, \n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# One time computation. \n",
    "for draft_chunk in pd.read_csv(csv_path, chunksize=chunk_size):\n",
    "\n",
    "    # Remove basics. \n",
    "    draft_chunk = remove_basics(draft_chunk)\n",
    "    \n",
    "    # Get date 1 week after start of draft (assumes drafts sorted by draft time). \n",
    "    first_date_str = draft_chunk[\"draft_time\"].min()\n",
    "    first_date_obj = datetime.strptime(first_date_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "    min_date_obj = first_date_obj + timedelta(days=7)\n",
    "    min_date_str = min_date_obj.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Get cardnames and ids. \n",
    "    pack_cols = [col for col in draft_chunk.columns if col.startswith(\"pack_card\")]\n",
    "    cardnames = [col[10:] for col in sorted(pack_cols)]\n",
    "    class_to_index = {cls: idx for idx, cls in enumerate(cardnames)}\n",
    "    \n",
    "    print(\"Completed one-time computation.\")\n",
    "    break\n",
    "\n",
    "for draft_chunk in pd.read_csv(csv_path, chunksize=chunk_size):\n",
    "\n",
    "    # Remove basics. \n",
    "    draft_chunk = remove_basics(draft_chunk)\n",
    "    \n",
    "    # Filtering. \n",
    "    draft_chunk = draft_chunk[draft_chunk[\"draft_time\"] > min_date_str] # Filter out first week. \n",
    "    draft_chunk = draft_chunk[draft_chunk[\"rank\"].isin([\"diamond\", \"mythic\"])] # Only highly ranked. \n",
    "    \n",
    "    # Extract packs. \n",
    "    pack_chunk = draft_chunk[sorted(pack_cols)].astype(bool)\n",
    "\n",
    "    # Extract pools. \n",
    "    pool_cols = [col for col in draft_chunk.columns if col.startswith(\"pool_\")]\n",
    "    pool_chunk = draft_chunk[sorted(pool_cols)].astype(np.uint8)\n",
    "    \n",
    "    # Extract picks. \n",
    "    pick_chunk = np.zeros((len(draft_chunk), len(cardnames)), dtype=bool)\n",
    "    for i, item in enumerate(draft_chunk[\"pick\"]):\n",
    "        pick_chunk[i, class_to_index[item]] = True\n",
    "\n",
    "    # Append data (consider multiple files for memory efficiency). \n",
    "    pick_chunks.append(pick_chunk)\n",
    "    pack_chunks.append(pack_chunk)\n",
    "    pool_chunks.append(pool_chunk)\n",
    "    \n",
    "    print(\"processed a chunk, t=\", round(time.time()-t0), \"s\")\n",
    "\n",
    "print(\"Done\")\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "picks = np.vstack(pick_chunks)\n",
    "packs = pd.concat(pack_chunks, ignore_index=True)\n",
    "pools = pd.concat(pool_chunks, ignore_index=True)\n",
    "\n",
    "print(packs.shape, pools.shape, picks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a6bd5c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible filters on strength: \n",
    "    # Rank\n",
    "    # Event match wins\n",
    "    # User_n_games, winrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "61f8f55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply filters. \n",
    "\n",
    "# # Platinum+ players. \n",
    "# # draft_chunk = raw_draft_chunk[raw_draft_chunk[\"rank\"].isin([\"diamond\", \"mythic\", \"platinum\"])]\n",
    "# draft_chunk = raw_draft_chunk # All ranks\n",
    "\n",
    "# # Filter out first week. \n",
    "# min_date_str = draft_chunk[\"draft_time\"].min()\n",
    "# date_obj = datetime.strptime(min_date_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "# new_date = date_obj + timedelta(days=7)\n",
    "# new_date_str = new_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# draft_chunk = draft_chunk[draft_chunk[\"draft_time\"] > new_date_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f60a3df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae67c3f",
   "metadata": {},
   "source": [
    "## Pick Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9b1c4358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Here's the MTG dataset class. \n",
    "class PickDataset(Dataset):\n",
    "    def __init__(self, pools, packs, pick_vectors, cardnames):\n",
    "        # Input is numpy arrays\n",
    "        self.pools = pools\n",
    "        self.packs = packs\n",
    "        self.pick_vectors = pick_vectors\n",
    "        self.cardnames = cardnames\n",
    "                            \n",
    "    def __len__(self):\n",
    "        return len(self.packs)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return torch.from_numpy(self.pools[index]), torch.from_numpy(self.packs[index]), torch.from_numpy(self.pick_vectors[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "258bf7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine train-val split. \n",
    "train_fraction = 0.8\n",
    "tsize = round(len(pools) * train_fraction)\n",
    "\n",
    "pick_train_dataset = PickDataset(pools[:tsize].values, packs[:tsize].values, picks[:tsize], cardnames)\n",
    "pick_train_dataloader = DataLoader(pick_train_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "pick_val_dataset = PickDataset(pools[tsize:].values, packs[tsize:].values, picks[tsize:], cardnames)\n",
    "pick_val_dataloader = DataLoader(pick_val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0d758bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how we trieve cardnames\n",
    "# pick_val_dataset.cardnames\n",
    "# pick_val_dataloader.dataset.cardnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9149d6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load pick data: 0.53s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "for batch in pick_val_dataset:\n",
    "    po, pa, pv = batch\n",
    "print(f\"Time to load pick data: {round(time.time() - t0, 2)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3e45653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize updated datasets. \n",
    "dataset_folder = \"/Users/danielbrooks/Desktop/Code/statistical-drafting/datasets/BLB/\"\n",
    "!mkdir -p $dataset_folder\n",
    "\n",
    "# Runs slowly... hmmm.\n",
    "torch.save(pick_train_dataset, dataset_folder + \"pick_train_dataset_test.pth\")\n",
    "torch.save(pick_val_dataset, dataset_folder + \"pick_val_dataset_test.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "22a489e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This serialization strategy is only recommended for files that I completely control. \n",
    "# ds = torch.load(dataset_folder + \"pick_val_dataset.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2019e0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7G\t/Users/danielbrooks/Desktop/Code/statistical-drafting/datasets/BLB//pick_train_dataset_3days.pth\r\n",
      "608M\t/Users/danielbrooks/Desktop/Code/statistical-drafting/datasets/BLB//pick_train_dataset_test.pth\r\n",
      "931M\t/Users/danielbrooks/Desktop/Code/statistical-drafting/datasets/BLB//pick_val_dataset.pth\r\n",
      "435M\t/Users/danielbrooks/Desktop/Code/statistical-drafting/datasets/BLB//pick_val_dataset_3days.pth\r\n",
      "160M\t/Users/danielbrooks/Desktop/Code/statistical-drafting/datasets/BLB//pick_val_dataset_test.pth\r\n",
      " 20G\t/Users/danielbrooks/Desktop/Code/statistical-drafting/datasets/BLB//siamese_train_dataset.pth\r\n",
      "4.2G\t/Users/danielbrooks/Desktop/Code/statistical-drafting/datasets/BLB//siamese_train_dataset_3days.pth\r\n"
     ]
    }
   ],
   "source": [
    "# May run into memory issues on larger datasets. \n",
    "!du -sh $dataset_folder/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50ff3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural net architecture: https://github.com/khakhalin/MTG/blob/master/bots/draftsimtools/nnet_architecture.py\n",
    "# MLP training script: https://github.com/khakhalin/MTG/blob/master/bots/train_nnet.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
