{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deckbuilding Model Evaluation\n",
    "\n",
    "This notebook evaluates the deckbuilding model performance on validation data by comparing predicted decks against human decks.\n",
    "\n",
    "## Metrics\n",
    "\n",
    "1. **Card-accuracy**: Percentage of individual cards that match between predicted and human decks\n",
    "2. **Difference distribution**: Histogram showing how many examples have N cards different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import statisticaldeckbuild as sdb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Enable inline plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set configuration\n",
    "SET_ABBREVIATION = \"FDN\"\n",
    "DRAFT_MODE = \"Premier\"\n",
    "\n",
    "# Paths\n",
    "VAL_DATASET_PATH = f\"../data/training_sets/{SET_ABBREVIATION}_{DRAFT_MODE}_deckbuild_val.pth\"\n",
    "MODEL_FOLDER = \"../data/models/\"\n",
    "CARDS_FOLDER = \"../data/cards/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded validation dataset:\n",
      "  Number of examples: 7483\n",
      "  Number of cards: 281\n",
      "  Deck shape: (7483, 281)\n",
      "  Sideboard shape: (7483, 281)\n"
     ]
    }
   ],
   "source": [
    "# Load validation dataset\n",
    "val_dataset = torch.load(VAL_DATASET_PATH, weights_only=False)\n",
    "\n",
    "print(f\"Loaded validation dataset:\")\n",
    "print(f\"  Number of examples: {len(val_dataset)}\")\n",
    "print(f\"  Number of cards: {len(val_dataset.cardnames)}\")\n",
    "print(f\"  Deck shape: {val_dataset.decks.shape}\")\n",
    "print(f\"  Sideboard shape: {val_dataset.sideboards.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Deckbuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'statisticaldeckbuild' has no attribute 'IterativeDeckBuilder'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Initialize the deck builder\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m builder = \u001b[43msdb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mIterativeDeckBuilder\u001b[49m(\n\u001b[32m      3\u001b[39m     set_abbreviation=SET_ABBREVIATION,\n\u001b[32m      4\u001b[39m     draft_mode=DRAFT_MODE,\n\u001b[32m      5\u001b[39m     model_folder=MODEL_FOLDER,\n\u001b[32m      6\u001b[39m     cards_folder=CARDS_FOLDER,\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInitialized IterativeDeckBuilder\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbuilder.device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'statisticaldeckbuild' has no attribute 'IterativeDeckBuilder'"
     ]
    }
   ],
   "source": [
    "# Initialize the deck builder\n",
    "builder = sdb.IterativeDeckBuilder(\n",
    "    set_abbreviation=SET_ABBREVIATION,\n",
    "    draft_mode=DRAFT_MODE,\n",
    "    model_folder=MODEL_FOLDER,\n",
    "    cards_folder=CARDS_FOLDER,\n",
    ")\n",
    "\n",
    "print(f\"Initialized IterativeDeckBuilder\")\n",
    "print(f\"  Device: {builder.device}\")\n",
    "print(f\"  Number of cards: {builder.num_cards}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Test - Evaluate on Small Subset\n",
    "\n",
    "First, let's run on a small subset to verify everything works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on small subset\n",
    "results_small = sdb.evaluate_deckbuilder(\n",
    "    val_dataset=val_dataset,\n",
    "    builder=builder,\n",
    "    max_examples=10,  # Quick test with just 10 examples\n",
    "    progress_interval=5,\n",
    "    verbose=True,\n",
    "    save_results=False,  # Don't save for quick test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary of small test\n",
    "sdb.print_summary(results_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medium Test - 100 Examples\n",
    "\n",
    "Now let's run on a larger subset to get more reliable statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on 100 examples\n",
    "results_medium = sdb.evaluate_deckbuilder(\n",
    "    val_dataset=val_dataset,\n",
    "    builder=builder,\n",
    "    max_examples=100,\n",
    "    progress_interval=10,\n",
    "    verbose=True,\n",
    "    save_results=True,  # Save results\n",
    "    output_path=f\"eval_{SET_ABBREVIATION}_{DRAFT_MODE}_100examples.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed summary\n",
    "sdb.print_summary(results_medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot difference distribution\n",
    "sdb.plot_difference_distribution(\n",
    "    results_medium,\n",
    "    save_path=f\"eval_{SET_ABBREVIATION}_{DRAFT_MODE}_100examples_distribution.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Individual Examples\n",
    "\n",
    "Let's look at some individual examples to understand what the model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at per-example results\n",
    "per_example = results_medium[\"per_example_results\"]\n",
    "\n",
    "# Sort by number of cards different\n",
    "sorted_examples = sorted(per_example, key=lambda x: x[\"num_different\"])\n",
    "\n",
    "print(\"Best 5 predictions (fewest cards different):\")\n",
    "for ex in sorted_examples[:5]:\n",
    "    print(f\"  Example {ex['index']:3d}: {ex['num_different']:2d} cards different, {ex['num_matches']:2d} matches\")\n",
    "\n",
    "print(\"\\nWorst 5 predictions (most cards different):\")\n",
    "for ex in sorted_examples[-5:]:\n",
    "    print(f\"  Example {ex['index']:3d}: {ex['num_different']:2d} cards different, {ex['num_matches']:2d} matches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Example Inspection\n",
    "\n",
    "Let's look at a specific example in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick an example to inspect (e.g., the best one)\n",
    "example_idx = sorted_examples[0][\"index\"]\n",
    "\n",
    "# Get the pool and build the deck\n",
    "pool = sdb.evaluate.pool_from_dataset_example(val_dataset, example_idx)\n",
    "result = builder.build_deck(pool, target_deck_size=23, verbose=True)\n",
    "\n",
    "print(f\"\\nExample {example_idx}:\")\n",
    "print(f\"Pool size: {len(pool)} cards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the predicted deck\n",
    "builder.print_deck_and_sideboard(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with human deck\n",
    "human_deck = val_dataset.decks[example_idx]\n",
    "predicted_deck = sdb.evaluate.predicted_deck_to_counts(result, val_dataset.cardnames)\n",
    "\n",
    "print(\"\\nComparison with human deck:\")\n",
    "print(f\"{'Card Name':<30} {'Human':>6} {'Predicted':>9} {'Match':>6}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for i, card_name in enumerate(val_dataset.cardnames):\n",
    "    if human_deck[i] > 0 or predicted_deck[i] > 0:\n",
    "        human_count = int(human_deck[i])\n",
    "        pred_count = int(predicted_deck[i])\n",
    "        match = \"✓\" if human_count == pred_count else \"✗\"\n",
    "        print(f\"{card_name:<30} {human_count:>6} {pred_count:>9} {match:>6}\")\n",
    "\n",
    "matches, total = sdb.compute_card_accuracy(predicted_deck, human_deck)\n",
    "print(f\"\\nMatches: {matches}/{total} ({100*matches/total:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Evaluation (Optional)\n",
    "\n",
    "Run evaluation on the full validation set. This may take a while depending on the dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run full evaluation\n",
    "# results_full = sdb.evaluate_deckbuilder(\n",
    "#     val_dataset=val_dataset,\n",
    "#     builder=builder,\n",
    "#     max_examples=None,  # Evaluate all examples\n",
    "#     progress_interval=100,\n",
    "#     verbose=True,\n",
    "#     save_results=True,\n",
    "#     output_path=f\"eval_{SET_ABBREVIATION}_{DRAFT_MODE}_full.json\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to print full results\n",
    "# sdb.print_summary(results_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to plot full results\n",
    "# sdb.plot_difference_distribution(\n",
    "#     results_full,\n",
    "#     save_path=f\"eval_{SET_ABBREVIATION}_{DRAFT_MODE}_full_distribution.png\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Conclusions\n",
    "\n",
    "Based on the evaluation results, analyze:\n",
    "- Is the card accuracy above 50%? (Random baseline would be near 0%)\n",
    "- What is the typical number of cards different?\n",
    "- Where does the difference distribution peak?\n",
    "- Are there any outliers with very high difference counts?\n",
    "\n",
    "Use this information to guide model improvements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
